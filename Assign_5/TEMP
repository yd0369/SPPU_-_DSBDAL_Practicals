
"""ass5.ipynb

Automatically generated by Colaboratory.

Original file is located at
https://colab.research.google.com/drive/1VR1VwVtXpJrNiMqkhajRdA-bftRxDx4k
"""

# We have social ad dataset
# 1) Purchased -> Output variable (Dependent variable)
# 0 -> Not purchased
# 1 -> Purchased

# Predict the given person having
# 1) Gender
# 2) Age
# 3) Salary
# Whether that person will purchase or NOT

# Yes / No -> It is called as classification problem

import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/shivang98/Social-Network-ads-Boost/master/Social_Network_Ads.csv')

df.sample(10)

# We have seen that, there are total 5 attributes / variables
# User ID -> Is not helpful for classification

df.drop(columns=['User ID'], inplace=True)
df.sample(10)

# We see Gender as a categorical data, so conver it into number (encoding)
df.dtypes

df['Gender'] = df['Gender'].astype('category')
df.dtypes

df['Gender'] = df['Gender'].cat.codes
df.sample(10)

df['Gender'].value_counts()

def DetectOutlier(df, var):
high, low = df[var].mean() + 3*df[var].std(), df[var].mean() - 3*df[var].std()
print("Highest allowed in Variable : ", var, high)
print("Lowest allowed in Variable: ", var, low)
count = df[(df[var] > high ) | (df[var] < low)][var].count()

 print('Total outliers in ', var, ':', count)

DetectOutlier(df,'Age')

DetectOutlier(df,'EstimatedSalary')

df.isna().sum()

from numpy.matrixlib.defmatrix import matrix
# Now, next step is to find out co-relation between input and output variable
# How do we do it?
# 1) Co-relation matrix
# 2) Ploting on Heatmap

import seaborn as sns
sns.heatmap(df.corr(), annot=True)

from numpy import testing
X = df[['Age', 'EstimatedSalary']]
Y = df['Purchased']

# Divide the data into training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Apply logistic regression
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train,Y_train)
print('Model Score: ', model.score(X_test, Y_test))

# This will print statistical information
df.describe()

# We see that there is huge range within the data, so the higher value data may inadverntly
# do a bossing in this classification
# If we have to give equal importance to all the variables (in terms of its value)
# Then use the techniqe called as normalization
# Normalization means keeping the data within the range [0 - 1]

X = df[['Age', 'EstimatedSalary']]
Y = df['Purchased']

# Divide the data into training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

from sklearn.preprocessing import MinMaxScaler

# fit scaler on training data
norm = MinMaxScaler().fit(X_train)

# transform training data
X_train = norm.transform(X_train)

# fit scaler on training data
norm = MinMaxScaler().fit(X_test)

# transform training data
X_test = norm.transform(X_test)

# Apply logistic regression
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train,Y_train)
print('Model Score: ', model.score(X_test, Y_test))