
# -*- coding: utf-8 -*-
"""ass4.ipynb

Automatically generated by Colaboratory.

Original file is located at
https://colab.research.google.com/drive/1Tpj6Mh7Rn6gWBHWaOwn7wMopxA8WStKx
"""

# Boston dataset

import pandas as pd
df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv')
df.head()

df.shape
# This dataset has 14 features
# 13 features are independent featurers (input features)
# 1-medv dependent feature (output feature)
# It has 506 data values (data rows)

# Let us see how many missing values are there?
df.isna().sum()

# Let us try to find out co-relation between input parameters and output parameter
# We are finding out dependency between input and output parameters

# co-relation matrix

import seaborn as sns

sns.heatmap(df.corr(), annot = True)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(16, 8))
sns.heatmap(df.corr(), annot = True)

# We found that RM and LSTAT are highly co-related with the medv
# Now let use use only two parameters for prediction

data = df[['rm', 'lstat', 'medv']]
data.head()

sns.boxplot(x=df['rm'])

sns.boxplot(x=df['lstat'])

sns.scatterplot(data=df, x="rm", y="medv")

sns.scatterplot(data=df, x="lstat", y="medv")

# We have understood relationship between
# 1) Number of Rooms
# 2) Lower class area

# Create the model which will predict the cost of the property
# 1) Divide the given data into training data and testing data ( 80% training and 20% testing)

#Nomenclature
# X -> Input data
# Y - Output data
# X_train => Input training data
# X_test => Input testing data
# Y_train => Output training data
# Y_test => Output testing data

# X_train, Y_train We will train the model

# X_test, Y_test we will test the model

X = df[['rm', 'lstat']] #Input data (independent data)
Y = df['medv'] # Output data (dependent data)

# What is scikit learn
# how it is useful in machine learning

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(X_test.shape)

print(X_train.head())
print(Y_train.head())

# Let us use linear regression model for training
# => It is a linear relationship between input and output parameters (variables)
# => Advantage => Easy
# => Disadvantage => It is affected by outliers

from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(X_train, Y_train)
output = model.predict(X_test)
print(output)

print(Y_test)

from sklearn.metrics import mean_absolute_error
print("MAE: ", mean_absolute_error(Y_test, output))
print("Model Score: ", model.score(X_test, Y_test))

# Remove the outliers in rm and lstat
# again train the model
# test the model
# find accuracy

# To remove the outliers we can apply standard deviation method

def OutlierRemoval(df, var):
high, low = df[var].mean() + 3*df[var].std(), df[var].mean() - 3*df[var].std()
print("Highest allowed in Variable : ", var, high)
print("Lowest allowed in Variable: ", var, low)
count = df[(df[var] > high ) | (df[var] < low)][var].count()

 print('Total outliers in ', var, ':', count)

 df = df[((df[var] >= low ) & (df[var] <= high))]
return df

df.shape
df = OutlierRemoval(df, 'rm')
df.shape

print(df.shape)
df = OutlierRemoval(df, 'lstat')
print(df.shape)

X = df[['rm', 'lstat']] #Input data (independent data)
Y = df['medv'] # Output data (dependent data)

# What is scikit learn
# how it is useful in machine learning

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

LinearRegressionModel(X_train, Y_train, X_test, Y_test)

def LinearRegressionModel(X_train, Y_train, X_test, Y_test):
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(X_train, Y_train)
output = model.predict(X_test)
from sklearn.metrics import mean_absolute_error
print("MAE: ", mean_absolute_error(Y_test, output))
print("Model Score: ", model.score(X_test, Y_test))

df = OutlierRemoval(df, 'ptratio')

df = OutlierRemoval(df, 'indus')

X = df[['rm', 'lstat', 'ptratio']] #Input data (independent data)
Y = df['medv'] # Output data (dependent data)

# What is scikit learn
# how it is useful in machine learning

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

LinearRegressionModel(X_train, Y_train, X_test, Y_test)